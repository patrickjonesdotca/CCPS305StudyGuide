<!DOCTYPE html>
<html class="jsEnabled togglingEnabled" dir="ltr" lang="en"><head>
		<title>Selection algorithm - Wikipedia, the free encyclopedia</title>
		<meta http-equiv="content-type" content="text/html; charset=UTF-8">
		<meta name="robots" content="noindex,nofollow">		<link rel="stylesheet" href="Selection_algorithm_files/load.css" type="text/css" media="all">		<meta name="viewport" content="initial-scale=1.0, user-scalable=no">
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png">		<script type="text/javascript">
			if( typeof mw === 'undefined' ) {
				mw = {};
			}
			var mwMobileFrontendConfig = {"messages":{"expand-section":"Show","collapse-section":"Hide","remove-results":"Back...","mobile-frontend-search-noresults":"No article titles match your search. Change your search, or press the keyboard search button to search inside articles.","mobile-frontend-search-help":"Type search term above and matching article titles will appear here.","contents-heading":"Contents","language-heading":"Read this article in","mobile-frontend-close-section":"Close this section","mobile-frontend-language-header":"This article is available in 5 languages","mobile-frontend-language-footer":"<a href=\"\/wiki\/Special:Special:MobileOptions\/Language\">Wikipedia is available in other languages.<\/a>","mobile-frontend-language-site-choose":"Search language","mobile-frontend-language-site-nomatches":"No matching languages"},"settings":{"scriptPath":"\/w","useFormatCookieName":"mf_mobileFormat","useFormatCookieDuration":-1,"useFormatCookieDomain":"en.wikipedia.org","useFormatCookiePath":"\/","stopMobileRedirectCookieName":"stopMobileRedirect","stopMobileRedirectCookieDuration":15552000,"stopMobileRedirectCookieDomain":".wikipedia.org","hookOptions":""}};
			document.documentElement.className = 'jsEnabled page-loading';		</script>
		<link rel="canonical" href="http://en.wikipedia.org/wiki/Selection_algorithm">
	</head>
	<body class="mobile mw-mf-checkboxes">
							<div id="mw-mf-header">
					<form id="mw-mf-searchForm" action="/w/index.php" class="search_bar" method="get">
							<img alt="Logo" id="mw-mf-logo" src="Selection_algorithm_files/W_logo_for_Mobile_Frontend.gif" width="35" height="22">
						<input value="Special:Search" name="title" type="hidden">
			<div id="mw-mf-sq" class="divclearable">
				<input name="search" id="mw-mf-search" size="22" autocomplete="off" maxlength="1024" class="search" placeholder="Type your search here..." type="search">
				<img src="Selection_algorithm_files/blank.gif" alt="Clear" class="clearlink" id="mw-mf-clearsearch" title="Clear">
			</div>
						<button id="goButton" class="goButton" type="submit">
				<img src="Selection_algorithm_files/blank.gif" alt="Go" title="Go">
			</button>
					</form>
									<div class="nav" id="nav">
				<div id="mw-mf-language-selection">
		Language:<br>
		<select id="languageselection"><option value="http://en.wikipedia.org/wiki/Selection_algorithm" selected="selected">English</option><option value="//fa.m.wikipedia.org/wiki/%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85_%D8%A7%D9%86%D8%AA%D8%AE%D8%A7%D8%A8">فارسی</option><option value="//id.m.wikipedia.org/wiki/Algoritma_seleksi">Bahasa Indonesia</option><option value="//ja.m.wikipedia.org/wiki/%E9%81%B8%E6%8A%9E%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0">日本語</option><option value="//pl.m.wikipedia.org/wiki/Selekcja_(informatyka)">polski</option></select>
	</div><br>
			<a href="http://en.m.wikipedia.org/wiki/Main_Page" id="homeButton" class="button">Home</a>
			<a href="http://en.m.wikipedia.org/wiki/Special:Random" id="randomButton" class="button">Random</a>
			</div>
		</div>
				<div id="results"><ul class="suggestions-results" title="No Results"><li class="suggestions-result">Type search term above and matching article titles will appear here.</li></ul></div>
		<div class="show" id="content_wrapper">
				<div id="content">
			<h1 id="firstHeading">Selection algorithm</h1>			
<p>In <a href="http://en.m.wikipedia.org/wiki/Computer_science" title="Computer science">computer science</a>, a <b>selection algorithm</b> is an <a href="http://en.m.wikipedia.org/wiki/Algorithm" title="Algorithm">algorithm</a> for finding the <i>k</i>th smallest number in a list (such a number is called the <i>k</i>th <i><a href="http://en.m.wikipedia.org/wiki/Order_statistic" title="Order statistic">order statistic</a></i>). This includes the cases of finding the <a href="http://en.m.wikipedia.org/wiki/Minimum" title="Minimum" class="mw-redirect">minimum</a>, <a href="http://en.m.wikipedia.org/wiki/Maximum" title="Maximum" class="mw-redirect">maximum</a>, and <a href="http://en.m.wikipedia.org/wiki/Median" title="Median">median</a> elements. There are <img class="tex" alt="O(n)" src="Selection_algorithm_files/7ba55e7c64a9405a0b39a1107e90ca94.png">, worst-case linear time, selection algorithms. Selection is a subproblem of more complex problems like the <a href="http://en.m.wikipedia.org/wiki/Nearest_neighbor_problem" title="Nearest neighbor problem" class="mw-redirect">nearest neighbor problem</a> and <a href="http://en.m.wikipedia.org/wiki/Shortest_path" title="Shortest path" class="mw-redirect">shortest path</a> problems.</p>
<p>The term "selection" is used in other contexts in computer science, 
including the stage of a genetic algorithm in which genomes are chosen 
from a population for later breeding; see <a href="http://en.m.wikipedia.org/wiki/Selection_%28genetic_algorithm%29" title="Selection (genetic algorithm)">Selection (genetic algorithm)</a>. This article addresses only the problem of determining order statistics.</p>
<div class="section"><h2 class="section_heading" id="section_1"><button>Show</button><span id="Selection_by_sorting">Selection by sorting</span></h2><div class="content_block" id="content_1">
<p>Selection can be <a href="http://en.m.wikipedia.org/wiki/Reduction_%28complexity%29" title="Reduction (complexity)">reduced</a> to <a href="http://en.m.wikipedia.org/wiki/Sorting_algorithm" title="Sorting algorithm">sorting</a>
 by sorting the list and then extracting the desired element. This 
method is efficient when many selections need to be made from a list, in
 which case only one initial, expensive sort is needed, followed by many
 cheap extraction operations. In general, this method requires O(<i>n</i> log <i>n</i>) time, where <i>n</i> is the length of the list.</p>
</div><a id="anchor_1" href="#section_1" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 id="section_2" class="section_heading openSection"><button class="openSection">Hide</button><span id="Linear_minimum.2Fmaximum_algorithms">Linear minimum/maximum algorithms</span></h2><div class="content_block openSection" id="content_2">
<p>Linear time algorithms to find minima or maxima work by iterating 
over the list and keeping track of the minimum or maximum element so 
far.</p>
</div><a id="anchor_2" href="#section_2" class="section_anchors back_to_top openSection">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_3"><button>Show</button><span id="Nonlinear_general_selection_algorithm">Nonlinear general selection algorithm</span></h2><div class="content_block" id="content_3">
<p>Using the same ideas used in minimum/maximum algorithms, we can 
construct a simple, but inefficient general algorithm for finding the <i>k</i>th smallest or <i>k</i>th largest item in a list, requiring O(<i>kn</i>) time, which is effective when <i>k</i>
 is small. To accomplish this, we simply find the most extreme value and
 move it to the beginning until we reach our desired index. This can be 
seen as an incomplete <a href="http://en.m.wikipedia.org/wiki/Selection_sort" title="Selection sort">selection sort</a>. Here is the minimum-based algorithm:</p>
<pre> <b>function</b> select(list[1..n], k)
     <b>for</b> i <b>from</b> 1 <b>to</b> k
         minIndex = i
         minValue = list[i]
         <b>for</b> j <b>from</b> i+1 <b>to</b> n
             <b>if</b> list[j] &lt; minValue
                 minIndex = j
                 minValue = list[j]
         swap list[i] and list[minIndex]
     <b>return</b> list[k]
</pre>
<p>Other advantages of this method are:</p>
<ul>
<li>After locating the <i>j</i>th smallest element, it requires only O(<i>j</i> + (<i>k</i>-<i>j</i>)<sup>2</sup>) time to find the <i>k</i>th smallest element, or only O(1) for <i>k</i> ≤ <i>j</i>.</li>
<li>It can be done with <a href="http://en.m.wikipedia.org/wiki/Linked_list" title="Linked list">linked list</a> data structures, whereas the one based on partition requires <a href="http://en.m.wikipedia.org/wiki/Random_access" title="Random access">random access</a>.</li>
</ul>
</div><a id="anchor_3" href="#section_3" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_4"><button>Show</button><span id="Partition-based_general_selection_algorithm">Partition-based general selection algorithm</span></h2><div class="content_block" id="content_4">
<p>A general selection algorithm that is efficient in practice, but has 
poor worst-case performance, was conceived by the inventor of <a href="http://en.m.wikipedia.org/wiki/Quicksort" title="Quicksort">quicksort</a>, <a href="http://en.m.wikipedia.org/wiki/C.A.R._Hoare" title="C.A.R. Hoare" class="mw-redirect">C.A.R. Hoare</a>, and is known as <b>Hoare's selection algorithm</b> or <b>quickselect</b>.</p>
<p>In quicksort, there is a subprocedure called partition that can, in linear time, group a list (ranging from indices <code>left</code> to <code>right</code>)
 into two parts, those less than a certain element, and those greater 
than or equal to the element. Here is pseudocode that performs a 
partition about the element <code>list[pivotIndex]</code>:</p>
<pre> <b>function</b> partition(list, left, right, pivotIndex)
     pivotValue := list[pivotIndex]
     swap list[pivotIndex] and list[right]  <i>// Move pivot to end</i>
     storeIndex := left
     <b>for</b> i <b>from</b> left <b>to</b> right-1 
         <b>if</b> list[i] &lt;= pivotValue
             swap list[storeIndex] and list[i]
             increment storeIndex
     swap list[right] and list[storeIndex]  <i>// Move pivot to its final place</i>
     <b>return</b> storeIndex
</pre>
<p>In quicksort, we recursively sort both branches, leading to best-case <a href="http://en.m.wikipedia.org/wiki/Big-O_notation" title="Big-O notation" class="mw-redirect">Ω</a>(<i>n</i> log <i>n</i>)
 time. However, when doing selection, we already know which partition 
our desired element lies in, since the pivot is in its final sorted 
position, with all those preceding it in sorted order and all those 
following it in sorted order. Thus a single recursive call locates the 
desired element in the correct partition:</p>
<pre> <b>function</b> select(list, left, right, k)
     <b>if</b> left = right // If the list contains only one element
         <b>return</b> list[left]  // Return that element
     select pivotIndex between left and right
     pivotNewIndex := partition(list, left, right, pivotIndex)
     pivotDist := pivotNewIndex - left + 1 
     // The pivot is in its final sorted position, 
     // so pivotDist reflects its 1-based position if list were sorted
     <b>if</b> pivotDist = k 
         <b>return</b> list[pivotNewIndex]
     <b>else if</b> k &lt; pivotDist 
         <b>return</b> select(list, left, pivotNewIndex - 1, k)
     <b>else</b>
         <b>return</b> select(list, pivotNewIndex + 1, right, k - pivotDist)
</pre>
<p>Note the resemblance to quicksort: just as the minimum-based 
selection algorithm is a partial selection sort, this is a partial 
quicksort, generating and partitioning only O(log <i>n</i>) of its O(<i>n</i>)
 partitions. This simple procedure has expected linear performance, and,
 like quicksort, has quite good performance in practice. It is also an <a href="http://en.m.wikipedia.org/wiki/In-place_algorithm" title="In-place algorithm">in-place algorithm</a>, requiring only constant memory overhead, since the <a href="http://en.m.wikipedia.org/wiki/Tail_recursion" title="Tail recursion" class="mw-redirect">tail recursion</a> can be eliminated with a loop like this:</p>
<pre> <b>function</b> select(list, left, right, k)
     <b>loop</b>
         select pivotIndex between left and right
         pivotNewIndex := partition(list, left, right, pivotIndex)
         pivotDist := pivotNewIndex - left + 1
         <b>if</b> pivotDist = k
             <b>return</b> list[pivotNewIndex]
         <b>else if</b> k &lt; pivotDist
             right := pivotNewIndex - 1
         <b>else</b>
             k := k - pivotDist
             left := pivotNewIndex + 1

</pre>
<p>Like quicksort, the performance of the algorithm is sensitive to the 
pivot that is chosen. If bad pivots are consistently chosen, this 
degrades to the minimum-based selection described previously, and so can
 require as much as O(<i>n</i><sup>2</sup>) time. David Musser describes
 a "median-of-3 killer" sequence that can force the well-known 
median-of-three pivot selection algorithm to fail with worst-case 
behavior (see <i><a href="#Introselect">Introselect</a></i> section below).</p>
</div><a id="anchor_4" href="#section_4" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_5"><button>Show</button><span id="Linear_general_selection_algorithm_-_Median_of_Medians_algorithm">Linear general selection algorithm - Median of Medians algorithm</span></h2><div class="content_block" id="content_5">
<table class="infobox" style="width: 22em;" cellspacing="5">
<caption class="" style="">Median of Medians</caption>
<tbody><tr class="">
<th scope="row" style="text-align: left;">Class</th>
<td class="" style=""><b>Selection algorithm</b></td>
</tr>
<tr class="">
<th scope="row" style="text-align: left;">Data structure</th>
<td class="" style=""><a href="http://en.m.wikipedia.org/wiki/Array_data_structure" title="Array data structure">Array</a></td>
</tr>
<tr class="">
<th scope="row" style="text-align: left;"><a href="http://en.m.wikipedia.org/wiki/Best,_worst_and_average_case" title="Best, worst and average case">Worst case performance</a></th>
<td class="" style=""><img class="tex" alt="O(n)" src="Selection_algorithm_files/7ba55e7c64a9405a0b39a1107e90ca94.png"></td>
</tr>
<tr class="">
<th scope="row" style="text-align: left;"><a href="http://en.m.wikipedia.org/wiki/Best,_worst_and_average_case" title="Best, worst and average case">Best case performance</a></th>
<td class="" style=""><img class="tex" alt="O(n)" src="Selection_algorithm_files/7ba55e7c64a9405a0b39a1107e90ca94.png"></td>
</tr>
<tr class="">
<th scope="row" style="text-align: left;"><a href="http://en.m.wikipedia.org/wiki/Best,_worst_and_average_case" title="Best, worst and average case">Worst case space complexity</a></th>
<td class="" style="">
<img class="tex" alt="O(1)" src="Selection_algorithm_files/5e079a28737d5dd019a3b8f6133ee55e.png"> auxiliary</td>
</tr>
</tbody></table>
<p>A worst-case linear algorithm for the general case of selecting the <i>k</i>th largest element was published by <a href="http://en.m.wikipedia.org/wiki/Manuel_Blum" title="Manuel Blum">Blum</a>, <a href="http://en.m.wikipedia.org/wiki/Robert_Floyd" title="Robert Floyd" class="mw-redirect">Floyd</a>, <a href="http://en.m.wikipedia.org/wiki/Vaughan_Ronald_Pratt" title="Vaughan Ronald Pratt" class="mw-redirect">Pratt</a>, <a href="http://en.m.wikipedia.org/wiki/Ron_Rivest" title="Ron Rivest">Rivest</a> and <a href="http://en.m.wikipedia.org/wiki/Robert_Tarjan" title="Robert Tarjan">Tarjan</a> in their 1973 paper "Time bounds for selection", sometimes called <b>BFPRT</b> after the last names of the authors. It is based on the quickselect algorithm and is also known as the <b>median-of-medians algorithm</b>.</p>
<p>Although quickselect is linear-time on average, it can require 
quadratic time with poor pivot choices (consider the case of pivoting 
around the smallest element at each step). The solution to make it O(n) 
in the <i>worst</i> case is to consistently find "good" pivots. A good 
pivot is one for which we can establish that a constant proportion of 
elements fall both below and above it.</p>
<p>The <i>Select</i> algorithm divides the list into groups of five 
elements. (Left over elements are ignored for now.) Then, for each group
 of five, the median is calculated (an operation that can potentially be
 made very fast if the five values can be loaded into registers and 
compared). (If sorting in-place, then these medians are moved into one 
contiguous block in the list.) <i>Select</i> is then called recursively on this sublist of <i>n</i>/5 elements to find their true median. Finally, the "median of medians" is chosen to be the pivot.</p>
<h3> <span class="mw-headline" id="Properties_of_pivot">Properties of pivot</span>
</h3>
<p>The chosen pivot is both less than and greater than half of the elements in the list of medians, which is around <img class="tex" alt="n/10" src="Selection_algorithm_files/dbec376f4bf3477c9141087ce5db9a18.png"> elements <img class="tex" alt="(1/2 * (n/5))" src="Selection_algorithm_files/02b98625e940f31859e5f398621b3a9b.png">
 for each half. Each of these elements is a median of 5, making it less 
than 2 other elements and greater than 2 other elements outside the 
block. Hence, the pivot is less than <img class="tex" alt="3(n/10)" src="Selection_algorithm_files/2050790ac297580a1f9418f7d4147fa4.png"> elements outside the block, and greater than another <img class="tex" alt="3(n/10)" src="Selection_algorithm_files/2050790ac297580a1f9418f7d4147fa4.png">
 elements outside the block. Thus the chosen median splits the elements 
somewhere between 30%/70% and 70%/30%, which assures worst-case linear 
behavior of the algorithm. To visualize:</p>
<table class="wikitable" border="1">
<caption>One iteration on the list {0,1,2,3,...99}</caption>
<tbody><tr>
<th></th>
<td bgcolor="gray">12</td>
<td></td>
<td bgcolor="gray">15</td>
<td></td>
<td bgcolor="gray">11</td>
<td></td>
<td bgcolor="gray">2</td>
<td></td>
<td bgcolor="gray">9</td>
<td></td>
<td bgcolor="gray">5</td>
<td></td>
<td bgcolor="gray">0</td>
<td></td>
<td bgcolor="gray">7</td>
<td></td>
<td bgcolor="gray">3</td>
<td></td>
<td bgcolor="gray">21</td>
<td></td>
<td bgcolor="gray">44</td>
<td></td>
<td bgcolor="gray">40</td>
<td></td>
<td bgcolor="gray">1</td>
<td></td>
<td bgcolor="gray">18</td>
<td></td>
<td bgcolor="gray">20</td>
<td></td>
<td bgcolor="gray">32</td>
<td></td>
<td bgcolor="gray">19</td>
<td></td>
<td bgcolor="gray">35</td>
<td></td>
<td bgcolor="gray">37</td>
<td></td>
<td bgcolor="gray">39</td>
</tr>
<tr>
<th></th>
<td bgcolor="gray">13</td>
<td></td>
<td bgcolor="gray">16</td>
<td></td>
<td bgcolor="gray">14</td>
<td></td>
<td bgcolor="gray">8</td>
<td></td>
<td bgcolor="gray">10</td>
<td></td>
<td bgcolor="gray">26</td>
<td></td>
<td bgcolor="gray">6</td>
<td></td>
<td bgcolor="gray">33</td>
<td></td>
<td bgcolor="gray">4</td>
<td></td>
<td bgcolor="gray">27</td>
<td></td>
<td bgcolor="white">49</td>
<td></td>
<td bgcolor="gray">46</td>
<td></td>
<td bgcolor="white">52</td>
<td></td>
<td bgcolor="gray">25</td>
<td></td>
<td bgcolor="white">51</td>
<td></td>
<td bgcolor="gray">34</td>
<td></td>
<td bgcolor="gray">43</td>
<td></td>
<td bgcolor="white">56</td>
<td></td>
<td bgcolor="white">72</td>
<td></td>
<td bgcolor="white">79</td>
</tr>
<tr>
<th>Medians</th>
<td bgcolor="gray">17</td>
<td></td>
<td bgcolor="gray">23</td>
<td></td>
<td bgcolor="gray">24</td>
<td></td>
<td bgcolor="gray">28</td>
<td></td>
<td bgcolor="gray">29</td>
<td></td>
<td bgcolor="gray">30</td>
<td></td>
<td bgcolor="gray">31</td>
<td></td>
<td bgcolor="gray">36</td>
<td></td>
<td bgcolor="gray">42</td>
<td></td>
<td bgcolor="red">47</td>
<td></td>
<td bgcolor="white">50</td>
<td></td>
<td bgcolor="white">55</td>
<td></td>
<td bgcolor="white">58</td>
<td></td>
<td bgcolor="white">60</td>
<td></td>
<td bgcolor="white">63</td>
<td></td>
<td bgcolor="white">65</td>
<td></td>
<td bgcolor="white">66</td>
<td></td>
<td bgcolor="white">67</td>
<td></td>
<td bgcolor="white">81</td>
<td></td>
<td bgcolor="white">83</td>
</tr>
<tr>
<th></th>
<td bgcolor="gray">22</td>
<td></td>
<td bgcolor="gray">45</td>
<td></td>
<td bgcolor="gray">38</td>
<td></td>
<td bgcolor="white">53</td>
<td></td>
<td bgcolor="white">61</td>
<td></td>
<td bgcolor="gray">41</td>
<td></td>
<td bgcolor="white">62</td>
<td></td>
<td bgcolor="white">82</td>
<td></td>
<td bgcolor="white">54</td>
<td></td>
<td bgcolor="white">48</td>
<td></td>
<td bgcolor="white">59</td>
<td></td>
<td bgcolor="white">57</td>
<td></td>
<td bgcolor="white">71</td>
<td></td>
<td bgcolor="white">78</td>
<td></td>
<td bgcolor="white">64</td>
<td></td>
<td bgcolor="white">80</td>
<td></td>
<td bgcolor="white">70</td>
<td></td>
<td bgcolor="white">76</td>
<td></td>
<td bgcolor="white">85</td>
<td></td>
<td bgcolor="white">87</td>
</tr>
<tr>
<th></th>
<td bgcolor="white">96</td>
<td></td>
<td bgcolor="white">95</td>
<td></td>
<td bgcolor="white">94</td>
<td></td>
<td bgcolor="white">86</td>
<td></td>
<td bgcolor="white">89</td>
<td></td>
<td bgcolor="white">69</td>
<td></td>
<td bgcolor="white">68</td>
<td></td>
<td bgcolor="white">97</td>
<td></td>
<td bgcolor="white">73</td>
<td></td>
<td bgcolor="white">92</td>
<td></td>
<td bgcolor="white">74</td>
<td></td>
<td bgcolor="white">88</td>
<td></td>
<td bgcolor="white">99</td>
<td></td>
<td bgcolor="white">84</td>
<td></td>
<td bgcolor="white">75</td>
<td></td>
<td bgcolor="white">90</td>
<td></td>
<td bgcolor="white">77</td>
<td></td>
<td bgcolor="white">93</td>
<td></td>
<td bgcolor="white">98</td>
<td></td>
<td bgcolor="white">91</td>
</tr>
</tbody></table>
<p>(red = "(one of the two possible) median of medians", gray = "number &lt; red", white = "number &gt; red")</p>
<p>5-tuples are shown here sorted by median, for clarity. Sorting the 
tuples is not necessary because we only need the median for use as pivot
 element.</p>
<p>Note that all elements above/left of the red (30% of the 100 
elements) are less, and all elements below/right of the red (another 30%
 of the 100 elements) are greater.</p>
<h3> <span class="mw-headline" id="Proof_of_O.28n.29_running_time">Proof of O(n) running time</span>
</h3>
<p>The median-calculating recursive call does not exceed worst-case 
linear behavior because the list of medians is 20% of the size of the 
list, while the other recursive call recurse on at most 70% of the list,
 making the running time</p>
<pre> T(<i>n</i>) ≤ T(<i>n</i>/5) + T(7<i>n</i>/10) + O(<i>n</i>)
</pre>
<p>The O(<i>n</i>) is for the partitioning work (we visited each element
 a constant number of times, in order to form them into O(n) groups and 
take each median in O(1) time). From this, one can then show that T(<i>n</i>)&nbsp;≤&nbsp;c*n*(1&nbsp;+&nbsp;(9/10)&nbsp;+&nbsp;(9/10)<sup>2</sup>&nbsp;+&nbsp;...)&nbsp;=&nbsp;O(<i>n</i>).</p>
<h3> <span class="mw-headline" id="Important_notes">Important notes</span>
</h3>
<p>Although this approach optimizes quite well, it is typically 
outperformed in practice by the expected linear algorithm with random 
pivot choices<sup class="Template-Fact" style="white-space:nowrap;">[<i><a href="http://en.m.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources from December 2009">citation needed</span></a></i>]</sup>.</p>
<p>The worst-case algorithm can construct a worst-case O(<i>n</i>&nbsp;log&nbsp;<i>n</i>) <a href="http://en.m.wikipedia.org/wiki/Quicksort" title="Quicksort">quicksort</a> algorithm, by using it to find the median at every step.</p>
</div><a id="anchor_5" href="#section_5" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_6"><button>Show</button><span id="Introselect">Introselect</span></h2><div class="content_block" id="content_6">
<p><a href="http://en.m.wikipedia.org/wiki/David_Musser" title="David Musser">David Musser</a>'s well-known <a href="http://en.m.wikipedia.org/wiki/Introsort" title="Introsort">introsort</a> achieves practical performance comparable to quicksort while preserving <i>O</i>(<i>n</i> log <i>n</i>) worst-case behavior by creating a hybrid of quicksort and <a href="http://en.m.wikipedia.org/wiki/Heapsort" title="Heapsort">heapsort</a>. In the same paper, Musser introduced an "introspective selection" algorithm, popularly called <b>introselect</b>,
 which combines Hoare's algorithm with the worst-case linear algorithm 
described above to achieve worst-case linear selection with performance 
similar to Hoare's algorithm.<sup id="cite_ref-0" class="reference"><a href="#cite_note-0"><span>[</span>1<span>]</span></a></sup>
 It works by optimistically starting out with Hoare's algorithm and only
 switching to the worst-time linear algorithm if it recurses too many 
times without making sufficient progress. Simply limiting the recursion 
to constant depth is not good enough, since this would make the 
algorithm switch on all sufficiently large lists. Musser discusses a 
couple of simple approaches:</p>
<ul>
<li>Keep track of the list of sizes of the subpartitions processed so far. If at any point <i>k</i> recursive calls have been made without halving the list size, for some small positive <i>k</i>, switch to the worst-case linear algorithm.</li>
<li>Sum the size of all partitions generated so far. If this exceeds the list size times some small positive constant <i>k</i>, switch to the worst-case linear algorithm. This sum is easy to track in a single scalar variable.</li>
</ul>
<p>Both approaches limit the recursion depth to <i>k</i> ⌈log <i>n</i>⌉ = <i>O</i>(log <i>n</i>) and the total running time to <i>O</i>(<i>n)</i>.
 The paper suggested that more research on introselect was forthcoming, 
but the author retired in 2007 without having published any such further
 research.</p>
</div><a id="anchor_6" href="#section_6" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_7"><button>Show</button><span id="Selection_as_incremental_sorting">Selection as incremental sorting</span></h2><div class="content_block" id="content_7">
<p>One of the advantages of the sort-and-index approach, as mentioned, is its ability to <a href="http://en.m.wikipedia.org/wiki/Amortized_analysis" title="Amortized analysis">amortize</a>
 the sorting cost over many subsequent selections. However, sometimes 
the number of selections that will be done is not known in advance, and 
may be either small or large. In these cases, we can adapt the 
algorithms given above to simultaneously select an element while <a href="http://en.m.wikipedia.org/wiki/Partial_sorting" title="Partial sorting">partially sorting</a> the list, thus accelerating future selections.</p>
<p>Both the selection procedure based on minimum-finding and the one 
based on partitioning can be seen as a form of partial sort. The 
minimum-based algorithm sorts the list up to the given index, and so 
clearly speeds up future selections, especially of smaller indexes. The 
partition-based algorithm does not achieve the same behaviour 
automatically, but can be adapted to remember its previous pivot choices
 and reuse them wherever possible, avoiding costly partition operations,
 particularly the top-level one. The list becomes gradually more sorted 
as more partition operations are done incrementally; no pivots are ever 
"lost." If desired, this same pivot list could be passed on to quicksort
 to reuse, again avoiding many costly partition operations.</p>
</div><a id="anchor_7" href="#section_7" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_8"><button>Show</button><span id="Using_data_structures_to_select_in_sublinear_time">Using data structures to select in sublinear time</span></h2><div class="content_block" id="content_8">
<p>Given an unorganized list of data, linear time (Ω(<i>n</i>)) is 
required to find the minimum element, because we have to examine every 
element (otherwise, we might miss it). If we organize the list, for 
example by keeping it sorted at all times, then selecting the <i>k</i>th largest element is trivial, but then insertion requires linear time, as do other operations such as combining two lists.</p>
<p>The strategy to find an order statistic in <a href="http://en.m.wikipedia.org/wiki/Sublinear_time" title="Sublinear time" class="mw-redirect">sublinear time</a>
 is to store the data in an organized fashion using suitable data 
structures that facilitate the selection. Two such data structures are 
tree-based structures and frequency tables.</p>
<p>When only the minimum (or maximum) is needed, a good approach is to use a <a href="http://en.m.wikipedia.org/wiki/Heap_%28data_structure%29" title="Heap (data structure)">heap</a>,
 which is able to find the minimum (or maximum) element in constant 
time, while all other operations, including insertion, are O(log <i>n</i>) or better. More generally, a <a href="http://en.m.wikipedia.org/wiki/Self-balancing_binary_search_tree" title="Self-balancing binary search tree">self-balancing binary search tree</a> can easily be augmented to make it possible to both insert an element and find the <i>k</i>th largest element in O(log <i>n</i>)
 time. We simply store in each node a count of how many descendants it 
has, and use this to determine which path to follow. The information can
 be updated efficiently since adding a node only affects the counts of 
its O(log <i>n</i>) ancestors, and tree rotations only affect the counts of the nodes involved in the rotation.</p>
<p>Another simple strategy is based on some of the same concepts as the <a href="http://en.m.wikipedia.org/wiki/Hash_table" title="Hash table">hash table</a>. When we know the range of values beforehand, we can divide that range into <i>h</i> subintervals and assign these to <i>h</i>
 buckets. When we insert an element, we add it to the bucket 
corresponding to the interval it falls in. To find the minimum or 
maximum element, we scan from the beginning or end for the first 
nonempty bucket and find the minimum or maximum element in that bucket. 
In general, to find the <i>k</i>th element, we maintain a count of the 
number of elements in each bucket, then scan the buckets from left to 
right adding up counts until we find the bucket containing the desired 
element, then use the expected linear-time algorithm to find the correct
 element in that bucket.</p>
<p>If we choose <i>h</i> of size roughly sqrt(<i>n</i>), and the input is close to uniformly distributed, this scheme can perform selections in expected O(sqrt(<i>n</i>))
 time. Unfortunately, this strategy is also sensitive to clustering of 
elements in a narrow interval, which may result in buckets with large 
numbers of elements (clustering can be eliminated through a good hash 
function, but finding the element with the <i>k</i>th largest hash value
 isn't very useful). Additionally, like hash tables this structure 
requires table resizings to maintain efficiency as elements are added 
and <i>n</i> becomes much larger than <i>h</i><sup>2</sup>. A useful 
case of this is finding an order statistic or extremum in a finite range
 of data. Using above table with bucket interval 1 and maintaining 
counts in each bucket is much superior to other methods. Such hash 
tables are like <a href="http://en.m.wikipedia.org/wiki/Frequency_tables" title="Frequency tables" class="mw-redirect">frequency tables</a> used to classify the data in <a href="http://en.m.wikipedia.org/wiki/Descriptive_statistics" title="Descriptive statistics">descriptive statistics</a>.</p>
</div><a id="anchor_8" href="#section_8" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_9"><button>Show</button><span id="Selecting_k_smallest_or_largest_elements">Selecting k smallest or largest elements</span></h2><div class="content_block" id="content_9">
<div class="rellink relarticle mainarticle">Main article: <a href="http://en.m.wikipedia.org/wiki/Partial_sorting" title="Partial sorting">Partial sorting</a>
</div>
<p>Another fundamental selection problem is that of selecting the <i>k</i> smallest or <i>k</i> largest elements, which is particularly useful where we want to present just the "top <i>k</i>" of an unsorted list, such as the top 100 corporations by gross sales. This is also commonly called <a href="http://en.m.wikipedia.org/wiki/Partial_sorting" title="Partial sorting">partial sorting</a>.</p>
</div><a id="anchor_9" href="#section_9" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_10"><button>Show</button><span id="Lower_bounds">Lower bounds</span></h2><div class="content_block" id="content_10">
<p>In <i><a href="http://en.m.wikipedia.org/wiki/The_Art_of_Computer_Programming" title="The Art of Computer Programming">The Art of Computer Programming</a></i>, Donald E. Knuth discussed a number of lower bounds for the number of comparisons required to locate the <i>t</i> smallest entries of an unorganized list of <i>n</i> items (using only comparisons). There is a trivial lower bound of <i>n</i>
 − 1 for the minimum or maximum entry. To see this, consider a 
tournament where each game represents one comparison. Since every player
 except the winner of the tournament must lose a game before we know the
 winner, we have a lower bound of <i>n</i> − 1 comparisons.</p>
<p>The story becomes more complex for other indexes. We define <img class="tex" alt="W_{t}(n)" src="Selection_algorithm_files/d738b2aeaba785e8a2869570263076a0.png"> as the minimum number of comparisons required to find the <i>t</i> smallest values. Knuth references a paper published by S. S. Kislitsyn, which shows an upper bound on this value:</p>
<dl>
<dd><img class="tex" alt="W_{t}(n) \leq n - t + \sum_{n+1-t &lt; j \leq n} \lceil{\log_2\, j}\rceil \quad for\, n \geq t" src="Selection_algorithm_files/11b60a7a1b90b9cfea82d9b22b603507.png"></dd>
</dl>
<p>This bound is achievable for <i>t</i>=2 but better, more complex bounds are known for larger <i>t</i>.</p>
</div><a id="anchor_10" href="#section_10" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_11"><button>Show</button><span id="Language_support">Language support</span></h2><div class="content_block" id="content_11">
<p>Very few languages have built-in support for general selection, 
although many provide facilities for finding the smallest or largest 
element of a list. A notable exception is <a href="http://en.m.wikipedia.org/wiki/C%2B%2B" title="C++">C++</a>, which provides a templated <code>nth_element</code>
 method with a guarantee of expected linear time. It is implied but not 
required that it is based on Hoare's algorithm by its requirement of 
expected linear time. (Ref section 25.3.2 of ISO/IEC 14882:2003(E) and 
14882:1998(E), see also <a rel="nofollow" class="external text" href="http://www.sgi.com/tech/stl/nth_element.html">SGI STL description of nth_element</a>)</p>
<p>C++ also provides the <a rel="nofollow" class="external text" href="http://www.sgi.com/tech/stl/partial_sort.html">partial_sort</a> algorithm, which solves the problem of selecting the smallest <i>k</i> elements (sorted), with a time complexity of O(<i>n</i> log <i>k</i>). No algorithm is provided for selecting the greatest <i>k</i> elements since this should be done by inverting the ordering <a href="http://en.m.wikipedia.org/wiki/Predicate_%28computer_programming%29" title="Predicate (computer programming)" class="mw-redirect">predicate</a>.</p>
<p>For <a href="http://en.m.wikipedia.org/wiki/Perl" title="Perl">Perl</a>, the module <a rel="nofollow" class="external text" href="http://search.cpan.org/%7Esalva/Sort-Key-Top">Sort::Key::Top</a>, available from <a href="http://en.m.wikipedia.org/wiki/CPAN" title="CPAN">CPAN</a>,
 provides a set of functions to select the top n elements from a list 
using several orderings and custom key extraction procedures.</p>
<p><a href="http://en.m.wikipedia.org/wiki/Python_%28programming_language%29" title="Python (programming language)">Python</a>'s standard library (since 2.4) includes <code><a rel="nofollow" class="external text" href="http://docs.python.org/library/heapq.html">heapq</a>.nsmallest()</code> and <code>nlargest()</code>, returning sorted lists, the former in O(<i>n</i> + <i>k</i> log <i>n</i>) time, the latter in O(<i>n</i> log <i>k</i>) time.</p>
<p>Because <a href="http://en.m.wikipedia.org/wiki/Sorting_algorithm#Language_support" title="Sorting algorithm">language support for sorting</a>
 is more ubiquitous, the simplistic approach of sorting followed by 
indexing is preferred in many environments despite its disadvantage in 
speed. Indeed for <a href="http://en.m.wikipedia.org/wiki/Lazy_evaluation" title="Lazy evaluation">lazy languages</a>, this simplistic approach can even achieve the best complexity possible for the <i>k</i> smallest/greatest sorted (with maximum/minimum as a special case) if the sort is lazy enough.</p>
</div><a id="anchor_11" href="#section_11" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_12"><button>Show</button><span id="Online_selection_algorithm">Online selection algorithm</span></h2><div class="content_block" id="content_12">
<p>In certain selection problems, selection must be online, that is, an 
element can only be selected from a sequential input at the instance of 
observation and each selection, respectively refusal, is irrevocable. 
The problem is to select, under these constraints, a specific element of
 the input sequence (as for example the largest or the smallest value) 
with largest probability. This problem can be tackled by the <a href="http://en.m.wikipedia.org/wiki/Odds_algorithm" title="Odds algorithm">Odds algorithm</a> designed by <a href="http://en.m.wikipedia.org/wiki/F._Thomas_Bruss" title="F. Thomas Bruss" class="mw-redirect">F. Thomas Bruss</a>
 who coined the name Odds algorithm. It is also known as Bruss-algorithm
 or Bruss-strategy. This algorithm yields the optimal under an 
independence condition; it is also optimal itself as an algorithm with 
the number of computations being linear in the length of input.</p>
</div><a id="anchor_12" href="#section_12" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_13"><button>Show</button><span id="Notes">Notes</span></h2><div class="content_block" id="content_13">
<ol class="references">
<li id="cite_note-0">
<span class="mw-cite-backlink"><b><a href="#cite_ref-0">^</a></b></span> <span class="reference-text">David R. Musser. Introspective Sorting and Selection Algorithms. <i>Software: Practice and Experience</i>, vol. 27, no. 8, pp.983–993. 1997. Section: Introspective Selection Algorithms.</span>
</li>
</ol>
</div><a id="anchor_13" href="#section_13" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_14"><button>Show</button><span id="References">References</span></h2><div class="content_block" id="content_14">
<ul>
<li>
<a href="http://en.m.wikipedia.org/wiki/Manuel_Blum" title="Manuel Blum">M. Blum</a>, <a href="http://en.m.wikipedia.org/wiki/Robert_Floyd" title="Robert Floyd" class="mw-redirect">R.W. Floyd</a>, <a href="http://en.m.wikipedia.org/wiki/Vaughan_Ronald_Pratt" title="Vaughan Ronald Pratt" class="mw-redirect">V. Pratt</a>, <a href="http://en.m.wikipedia.org/wiki/Ron_Rivest" title="Ron Rivest">R. Rivest</a> and <a href="http://en.m.wikipedia.org/wiki/Robert_Tarjan" title="Robert Tarjan">R. Tarjan</a>, "Time bounds for selection," <i>J. Comput. System Sci</i>. 7 (1973) 448-461.</li>
<li>K. C. Kiwiel. On Floyd and Rivest’s SELECT Algorithm, <i>Theoretical Computer Sci.</i> 347 (2005) 214-238.</li>
<li>
<a href="http://en.m.wikipedia.org/wiki/Donald_Knuth" title="Donald Knuth">Donald Knuth</a>. <i><a href="http://en.m.wikipedia.org/wiki/The_Art_of_Computer_Programming" title="The Art of Computer Programming">The Art of Computer Programming</a></i>, Volume 3: <i>Sorting and Searching</i>, Third Edition. Addison-Wesley, 1997. <a href="http://en.m.wikipedia.org/wiki/Special:BookSources/0201896850" class="internal mw-magiclink-isbn">ISBN 0-201-89685-0</a>. Section 5.3.3: Minimum-Comparison Selection, pp.207–219.</li>
<li>
<a href="http://en.m.wikipedia.org/wiki/Thomas_H._Cormen" title="Thomas H. Cormen">Thomas H. Cormen</a>, <a href="http://en.m.wikipedia.org/wiki/Charles_E._Leiserson" title="Charles E. Leiserson">Charles E. Leiserson</a>, <a href="http://en.m.wikipedia.org/wiki/Ronald_L._Rivest" title="Ronald L. Rivest" class="mw-redirect">Ronald L. Rivest</a>, and <a href="http://en.m.wikipedia.org/wiki/Clifford_Stein" title="Clifford Stein">Clifford Stein</a>. <i><a href="http://en.m.wikipedia.org/wiki/Introduction_to_Algorithms" title="Introduction to Algorithms">Introduction to Algorithms</a></i>, Second Edition. MIT Press and McGraw-Hill, 2001. <a href="http://en.m.wikipedia.org/wiki/Special:BookSources/0262032937" class="internal mw-magiclink-isbn">ISBN 0-262-03293-7</a>. Chapter 9: Medians and Order Statistics, pp.183–196. Section 14.1: Dynamic order statistics, pp.302–308.</li>
<li>Paul E. Black, <a rel="nofollow" class="external text" href="http://www.nist.gov/dads/HTML/select.html">Select</a> at the <a href="http://en.m.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" title="National Institute of Standards and Technology">NIST</a> <a href="http://en.m.wikipedia.org/wiki/Dictionary_of_Algorithms_and_Data_Structures" title="Dictionary of Algorithms and Data Structures">Dictionary of Algorithms and Data Structures</a>.</li>
</ul>
</div><a id="anchor_14" href="#section_14" class="section_anchors back_to_top">↑Jump back a section</a></div><div class="section"><h2 class="section_heading" id="section_15"><button>Show</button><span id="External_links">External links</span></h2><div class="content_block" id="content_15">
<ul>
<li>
<a rel="nofollow" class="external text" href="http://www.ics.uci.edu/%7Eeppstein/161/960130.html">Design and Analysis of Algorithms</a>, for a detailed explanation of the recurrence relation for the median-of-medians</li>
</ul>

</div>
</div>		</div>
	</div>
			<div id="footer">
			<h2 class="section_heading" id="section_footer"><button>Show</button>
		<div class="license">Wikipedia ™</div>		<span class="toggleCopyright">
			<span class="more">more</span><span class="less">less</span>
		</span>
	</h2>
	<div class="content_block" id="content_footer">
		<ul class="settings">
			<li>
				<span class="left separator"><a id="mw-mf-display-toggle" href="http://en.wikipedia.org/w/index.php?title=Selection_algorithm&amp;mobileaction=toggle_view_desktop">Desktop</a></span><span class="right">Mobile</span>
			</li>
			<li>
				<span class="left"><a href="http://wikimediafoundation.org/wiki/Terms_of_use?useformat=mobile">Terms of Use</a></span><span class="right">Images ON / <a href="http://en.m.wikipedia.org/w/index.php?title=Special:MobileOptions/DisableImages&amp;returnto=Selection+algorithm&amp;mobiletoken=5c89e355d2af09275b3b2ed3653e1683" title="Special:MobileOptions/DisableImages" id="imagetoggle">OFF</a></span>
			</li>
			<li class="notice">
				Article by <a href="http://en.m.wikipedia.org/w/index.php?title=Selection_algorithm&amp;action=history">contributors</a> like you<br>
				Content available under <a href="http://en.m.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License?useformat=mobile">CC BY-SA 3.0</a>			</li>
		</ul>
		<ul class="links">
			<li>
				<a href="http://en.m.wikipedia.org/w/index.php?title=Special:MobileFeedback&amp;returnto=Selection+algorithm&amp;feedbacksource=MobileFrontend">Contact</a>
			</li><li>
			<a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy</a></li><li>
			<a href="http://en.m.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About</a></li><li>
			<a href="http://en.m.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		</ul>
	</div>
	</div>
				<!--[if gt IE 7]><!-->
		<script type="text/javascript">mw={loader:{state:function(){}}};</script>		<script src="Selection_algorithm_files/load_002.php" type="text/javascript"></script>
<script src="Selection_algorithm_files/load.php" type="text/javascript"></script>	<script type="text/javascript">
	window.onload = function() {
		mw.mobileFrontend.init();
	};
	</script>
	<!----><!--[endif]---->
	
	<div style="display: none;" id="mf-references"><div></div><button>close</button></div></body></html>